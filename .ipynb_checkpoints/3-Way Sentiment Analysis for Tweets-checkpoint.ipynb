{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Way Sentiment Analysis for Tweets\n",
    "\n",
    "#### Overview\n",
    "\n",
    "In this project, we'll build a 3-way polarity (positive, negative, neutral) classification system for tweets, without using NLTK's in-built sentiment analysis engine. \n",
    "\n",
    "We'll use a logistic regression classifier, bag-of-words features, and polarity lexicons (both in-built and external). We'll also create our own pre-processing module to handle raw tweets. \n",
    "\n",
    "#### Data Used\n",
    "\n",
    "- training.json: This file contains ~15k raw tweets, along with their polarity labels (1 = positive, 0 = neutral, -1 = negative). We'll use this file to train our classifiers.\n",
    "\n",
    "- develop.json: In the same format as training.json, the file contains a smaller set of tweets. We'll use it to test the predictions of our classifiers which were trained on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The first thing that we'll do is preprocess the tweets so that they're easier to deal with, and ready for feature extraction, and training by the classifiers.\n",
    "\n",
    "To start with we're going to extract the tweets from the json file, read each line and store the tweets, labels in separate lists.\n",
    "\n",
    "Then for the preprocessing, we'll:\n",
    "\n",
    "- __segment__ tweets into sentences using an NTLK segmenter\n",
    "- __tokenize__ the sentences using an NLTK tokenizer\n",
    "- __lowercase__ all the words\n",
    "- __remove twitter usernames__ beginning with @ using regex\n",
    "- __remove URLs__ starting with http using regex\n",
    "- __process hashtags__ ,for this we'll tokenize hashtags,and try to break down multi-word hashtags using a MaxMatch algorithm, and the English word dictionary supplied with NLTK.\n",
    "\n",
    "Let's build some functions to accomplish all this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "dictionary = set(nltk.corpus.words.words()) #To be used for MaxMatch\n",
    "\n",
    "#Function to lemmatize word | Used during maxmatch\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "#Function to implement the maxmatch algorithm for multi-word hashtags\n",
    "def maxmatch(word,dictionary):\n",
    "    if not word:\n",
    "        return []\n",
    "    for i in range(len(word),1,-1):\n",
    "        first = word[0:i]\n",
    "        rem = word[i:]\n",
    "        if lemmatize(first).lower() in dictionary: #Important to lowercase lemmatized words before comparing in dictionary. \n",
    "            return [first] + maxmatch(rem,dictionary)\n",
    "    first = word[0:1]\n",
    "    rem = word[1:]\n",
    "    return [first] + maxmatch(rem,dictionary)\n",
    "\n",
    "#Function to preprocess a single tweet\n",
    "def preprocess(tweet):\n",
    "    \n",
    "    tweet = re.sub(\"@\\w+\",\"\",tweet).strip()\n",
    "    tweet = re.sub(\"http\\S+\",\"\",tweet).strip()\n",
    "    hashtags = re.findall(\"#\\w+\",tweet)\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"#\\w+\",\"\",tweet).strip() \n",
    "    \n",
    "    hashtag_tokens = [] #Separate list for hashtags\n",
    "    \n",
    "    for hashtag in hashtags:\n",
    "        hashtag_tokens.append(maxmatch(hashtag[1:],dictionary))        \n",
    "    \n",
    "    segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    segmented_sentences = segmenter.tokenize(tweet)\n",
    "    \n",
    "    #General tokenization\n",
    "    processed_tweet = []\n",
    "    \n",
    "    word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "    for sentence in segmented_sentences:\n",
    "        tokenized_sentence = word_tokenizer.tokenize(sentence.strip())\n",
    "        processed_tweet.append(tokenized_sentence)\n",
    "    \n",
    "    #Processing the hashtags only when they exist in a tweet\n",
    "    if hashtag_tokens:\n",
    "        for tag_token in hashtag_tokens:\n",
    "            processed_tweet.append(tag_token)\n",
    "    \n",
    "    return processed_tweet\n",
    "    \n",
    "#Custom function that takes in a file, and passes each tweet to the preprocessor\n",
    "def preprocess_file(filename):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        tweets.append(preprocess(tweet_dict[\"text\"]))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "    return tweets,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run preprocess our training data, let's see how well the maxmatch algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'can']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxmatch('wecan',dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try feeding it something harder than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cases', 'tu', 'd', 'y']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxmatch('casestudy',dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above example, it incorrectly breks down the word 'casestudy', by returning 'cases', instead of 'case' for the first iteration., which would have been a better output. This is because it _greedily_ extract 'cases' first.\n",
    "\n",
    "For an improvement, we can implement an algorithm that better counts the total number of successful matches in the result of the maxmatch process, and return the one with the highest successful match count.\n",
    "\n",
    "Let's run our preprocessing module on the raw training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Running the basic preprocessing module and capturing the data (maybe shift to the next block)\n",
    "train_data = preprocess_file('data/sentiment/training.json')\n",
    "train_tweets = train_data[0]\n",
    "train_labels = train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the first couple processed tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'dear', u'the', u'newooffice', u'for', u'mac', u'is', u'great', u'and', u'all', u',', u'but', u'no', u'lync', u'update', u'?'], [u'c', u\"'\", u'mon', u'.']], [[u'how', u'about', u'you', u'make', u'a', u'system', u'that', u'doesn', u\"'\", u't', u'eat', u'my', u'friggin', u'discs', u'.'], [u'this', u'is', u'the', u'2nd', u'time', u'this', u'has', u'happened', u'and', u'i', u'am', u'so', u'sick', u'of', u'it', u'!']]]\n"
     ]
    }
   ],
   "source": [
    "print train_tweets[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. we can do better than that to make sense of what's happening. Let's write a simple script to that'll run the preprocessing module on a few tweets, and print the original and processed results, side by side; if it detects a multi-word hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Original Tweet: If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft\n",
      "Processed tweet: [[u'if', u'i', u'make', u'a', u'game', u'as', u'a', u'universal', u'app', u'.'], [u'will', u'owners', u'be', u'able', u'to', u'download', u'and', u'play', u'it', u'in', u'november', u'?'], [u'windows', u'1', u'0'], [u'x', u'box', u'one']]\n",
      "\n",
      "2. Original Tweet: Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft\n",
      "Processed tweet: [[u'microsoft', u',', u'i', u'may', u'not', u'prefer', u'your', u'gaming', u'branch', u'of', u'business', u'.'], [u'but', u',', u'you', u'do', u'make', u'a', u'damn', u'fine', u'operating', u'system', u'.'], [u'Window', u's', u'1', u'0']]\n",
      "\n",
      "3. Original Tweet: @MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail\n",
      "Processed tweet: [[u'i', u'will', u'be', u'downgrading', u'and', u'let', u'be', u'out', u'for', u'almost', u'the', u'1st', u'yr', u'b4', u'trying', u'it', u'again', u'.'], [u'Window', u's', u'1', u'0'], [u'Window', u's', u'1', u'0', u'fail']]\n",
      "\n",
      "4. Original Tweet: @Microsoft 2nd computer with same error!!! #Windows10fail Guess we will shelve this until SP1! http://t.co/QCcHlKuy8Q\n",
      "Processed tweet: [[u'2nd', u'computer', u'with', u'same', u'error', u'!!!'], [u'guess', u'we', u'will', u'shelve', u'this', u'until', u'sp1', u'!'], [u'Window', u's', u'1', u'0', u'fail']]\n",
      "\n",
      "5. Original Tweet: Sunday morning, quiet day so time to welcome in #Windows10 @Microsoft @Windows http://t.co/7VtvAzhWmV\n",
      "Processed tweet: [[u'sunday', u'morning', u',', u'quiet', u'day', u'so', u'time', u'to', u'welcome', u'in'], [u'Window', u's', u'1', u'0']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing examples of multi-word hashtags (Doesn't work for multi sentence tweets)\n",
    "f = open('data/sentiment/training.json')\n",
    "count = 1\n",
    "for index,line in enumerate(f):\n",
    "    if count >5:\n",
    "        break\n",
    "    original_tweet = json.loads(line)[\"text\"]\n",
    "    hashtags = re.findall(\"#\\w+\",original_tweet)\n",
    "    if hashtags:\n",
    "        for hashtag in hashtags:\n",
    "            if len(maxmatch(hashtag[1:],dictionary)) > 1:\n",
    "                #If the length of the array returned by the maxmatch function is greater than 1,\n",
    "                #it means that the algorithm has detected a hashtag with more than 1 word inside. \n",
    "                print str(count) + \". Original Tweet: \" + original_tweet + \"\\nProcessed tweet: \" + str(train_tweets[index]) + \"\\n\"\n",
    "                count += 1\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! Our pre-processing module is working as intended.\n",
    "\n",
    "The next step is to convert each processed tweet into a bag-of-words feature dictionary. We'll allow for options to remove stopwords during the process, and also to remove _rare_ words, i.e. words occuring less than n times across the whole training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#To identify words appearing less than n times, we're creating a dictionary for the whole training set\n",
    "\n",
    "total_train_bow = {}\n",
    "\n",
    "for tweet in train_tweets:\n",
    "    for segment in tweet:\n",
    "        for token in segment:\n",
    "            total_train_bow[token] = total_train_bow.get(token,0) + 1\n",
    "\n",
    "#Function to convert pre_processed tweets to bag of words feature dictionaries\n",
    "#Allows for options to remove stopwords, and also to remove words occuring less than n times in the whole training set.            \n",
    "def convert_to_feature_dicts(tweets,remove_stop_words,n): \n",
    "    feature_dicts = []\n",
    "    for tweet in tweets:\n",
    "        # build feature dictionary for tweet\n",
    "        feature_dict = {}\n",
    "        if remove_stop_words:\n",
    "            for segment in tweet:\n",
    "                for token in segment:\n",
    "                    if token not in stopwords and (n<=0 or total_train_bow[token]>=n):\n",
    "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
    "        else:\n",
    "            for segment in tweet:\n",
    "                for token in segment:\n",
    "                    if n<=0 or total_train_bow[token]>=n:\n",
    "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
    "        feature_dicts.append(feature_dict)\n",
    "    return feature_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
