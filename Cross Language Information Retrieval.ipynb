{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross Language Information Retrieval\n",
    "\n",
    "#### Overview\n",
    "\n",
    "The aim of this project is to build a cross language information retrieval system (CLIR) which, given a query in German, will be capable of searching text documents written in English and displaying the results in German.\n",
    "\n",
    "We're going to use machine translation, information retrieval using a vector space model, and then assess the performance of the system using IR evaluation techniques.\n",
    "\n",
    "Parts of the project are explained as we progress.\n",
    "\n",
    "#### Data Used\n",
    "\n",
    "- bitext.(en,de): A sentence aligned, parallel German-English corpus, sourced from the Europarl corpus (which is a collection of debates held in the EU parliament over a number of years). We'll use this to develop word-alignment tools, and build a translation probability table. \n",
    "\n",
    "- newstest.(en,de): A separate, smaller parallel corpus for evaulation of the translation system.\n",
    "\n",
    "- devel.(docs,queries,qrel): A set of documents in English (sourced from Wikipedia), queries in German, and relevance judgement scores for each query-document pair. \n",
    "\n",
    "The files are available to check out in the data/clir directory of the repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping: File encodings and tokenisation\n",
    "\n",
    "Since the data files we use is utf-8 encoded text, we need to convert the strings into ASCII by escaping the special symbols. We also import some libraries in this step as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from __future__ import division #To properly handle floating point divisions.\n",
    "import math\n",
    "\n",
    "def tokenize(line, tokenizer=word_tokenize):\n",
    "    utf_line = line.decode('utf-8').lower()\n",
    "    return [token.encode('ascii', 'backslashreplace') for token in tokenizer(utf_line)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test out our tokenize function. Notice how it converts the word Über."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seit',\n",
       " 'damals',\n",
       " 'ist',\n",
       " 'er',\n",
       " 'auf',\n",
       " '\\\\xfcber',\n",
       " '10.000',\n",
       " 'punkte',\n",
       " 'gestiegen',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Seit damals ist er auf über 10.000 Punkte gestiegen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, lets get to the meat of the project. \n",
    "\n",
    "As mentioned earlier, we're going to build a CLIR engine consisting of information retrieval and translation components, and then evaluate its accuracy.\n",
    "\n",
    "The CLIR system will:\n",
    "- **translate queries** from German into English (because our searcheable corpus is in English), using word-based translation, a rather simplistic approach as opposed to the sophistication you might see in, say, *Google Translate*.\n",
    "- **search over the document corpus** using the Okapi BM25 IR ranking model, a variation of the traditional TF-IDF model.\n",
    "- **evaluate the quality** of ranked retrieval results using the query relevance judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval using [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25)\n",
    "\n",
    "We'll start by building an IR system, and give it a test run with some English queries. \n",
    "\n",
    "Here's an overview of the tasks involved:\n",
    "- Loading the data files, and tokenizing the input.\n",
    "- Preprocessing the lexicon by stemming, removing stopwords.\n",
    "- Calculating the TF/IDF representation for all documents in our wikipedia corpus.\n",
    "- Storing an inverted index to efficiently documents, given a query term.\n",
    "- Implementing querying with BM25.\n",
    "- Test runs.\n",
    "\n",
    "So for our first task, we'll load the devel.docs file, extract and tokenize the terms, and store them in a python dictionary with the document ids as keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) #converting stopwords to a set for faster processing in the future.\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "#Function to extract and tokenize terms from a document\n",
    "def extract_and_tokenize_terms(doc):\n",
    "    terms = []\n",
    "    for token in tokenize(doc):\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are faster over sets than lists\n",
    "            if not re.search(r'\\d',token) and not re.search(r'[^A-Za-z-]',token): #Removing numbers and punctuations \n",
    "                #(excluding hyphenated words)\n",
    "                terms.append(stemmer.stem(token.lower()))\n",
    "    return terms\n",
    "\n",
    "documents = {} #Dictionary to store documents with ids as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading each line in the file and storing it documents dictionary\n",
    "f = open('data/clir/devel.docs')\n",
    "for line in f:\n",
    "    doc = line.split(\"\\t\")\n",
    "    terms = extract_and_tokenize_terms(doc[1])\n",
    "    documents[doc[0]] = terms\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if everything is working till now, let's access a document from the dictionary, with the id '290'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'name',\n",
       " u'plural',\n",
       " u'ae',\n",
       " u'first',\n",
       " u'letter',\n",
       " u'vowel',\n",
       " u'iso',\n",
       " u'basic',\n",
       " u'latin',\n",
       " u'alphabet',\n",
       " u'similar',\n",
       " u'ancient',\n",
       " u'greek',\n",
       " u'letter',\n",
       " u'alpha',\n",
       " u'deriv',\n",
       " u'upper',\n",
       " u'case',\n",
       " u'version',\n",
       " u'consist',\n",
       " u'two',\n",
       " u'less',\n",
       " u'vertic',\n",
       " u'line',\n",
       " u'join',\n",
       " u'top',\n",
       " u'cross',\n",
       " u'middl',\n",
       " u'horizont',\n",
       " u'bar',\n",
       " u'originsth',\n",
       " u'earliest',\n",
       " u'certain',\n",
       " u'ancestor',\n",
       " u'aleph',\n",
       " u'also',\n",
       " u'call',\n",
       " u'aleph',\n",
       " u'first',\n",
       " u'letter',\n",
       " u'phoenician',\n",
       " u'alphabet',\n",
       " u'consist',\n",
       " u'entir',\n",
       " u'conson',\n",
       " u'abjad',\n",
       " u'rather',\n",
       " u'true',\n",
       " u'alphabet',\n",
       " u'turn',\n",
       " u'origin',\n",
       " u'aleph',\n",
       " u'may',\n",
       " u'pictogram',\n",
       " u'ox',\n",
       " u'head',\n",
       " u'proto',\n",
       " u'sinait',\n",
       " u'script',\n",
       " u'influenc',\n",
       " u'egyptian',\n",
       " u'hieroglyph',\n",
       " u'style',\n",
       " u'triangular',\n",
       " u'head',\n",
       " u'two',\n",
       " u'horn',\n",
       " u'extend',\n",
       " u'egyptian',\n",
       " u'cretan',\n",
       " u'phoenician',\n",
       " u'aleph',\n",
       " u'semit',\n",
       " u'greek',\n",
       " u'alpha',\n",
       " u'etruscan',\n",
       " u'roman',\n",
       " u'cyril',\n",
       " u'boeotian',\n",
       " u'--',\n",
       " u'bc',\n",
       " u'greek',\n",
       " u'uncial',\n",
       " u'latin',\n",
       " u'ad',\n",
       " u'uncial',\n",
       " u'phoenician',\n",
       " u'alphabet',\n",
       " u'letter',\n",
       " u'linear',\n",
       " u'form',\n",
       " u'serv',\n",
       " u'base',\n",
       " u'later',\n",
       " u'form',\n",
       " u'name',\n",
       " u'must',\n",
       " u'correspond',\n",
       " u'close',\n",
       " u'hebrew',\n",
       " u'arab',\n",
       " u'aleph',\n",
       " u'blacklett',\n",
       " u'uncial',\n",
       " u'anoth',\n",
       " u'blacklett',\n",
       " u'modern',\n",
       " u'roman',\n",
       " u'modern',\n",
       " u'ital',\n",
       " u'modern',\n",
       " u'script',\n",
       " u'ancient',\n",
       " u'greek',\n",
       " u'adopt',\n",
       " u'alphabet',\n",
       " u'use',\n",
       " u'glottal',\n",
       " u'stop',\n",
       " u'--',\n",
       " u'first',\n",
       " u'phonem',\n",
       " u'phoenician',\n",
       " u'pronunci',\n",
       " u'letter',\n",
       " u'sound',\n",
       " u'letter',\n",
       " u'denot',\n",
       " u'phoenician',\n",
       " u'semit',\n",
       " u'languag',\n",
       " u'--',\n",
       " u'use',\n",
       " u'adapt',\n",
       " u'sign',\n",
       " u'repres',\n",
       " u'vowel',\n",
       " u'gave',\n",
       " u'similar',\n",
       " u'name',\n",
       " u'alpha',\n",
       " u'earliest',\n",
       " u'greek',\n",
       " u'inscript',\n",
       " u'greek',\n",
       " u'dark',\n",
       " u'age',\n",
       " u'date',\n",
       " u'centuri',\n",
       " u'bc',\n",
       " u'letter',\n",
       " u'rest',\n",
       " u'upon',\n",
       " u'side',\n",
       " u'greek',\n",
       " u'alphabet',\n",
       " u'later',\n",
       " u'time',\n",
       " u'gener',\n",
       " u'resembl',\n",
       " u'modern',\n",
       " u'capit',\n",
       " u'letter',\n",
       " u'although',\n",
       " u'mani',\n",
       " u'local',\n",
       " u'varieti',\n",
       " u'distinguish',\n",
       " u'shorten',\n",
       " u'one',\n",
       " u'leg',\n",
       " u'angl',\n",
       " u'cross',\n",
       " u'line',\n",
       " u'set',\n",
       " u'etruscan',\n",
       " u'brought',\n",
       " u'greek',\n",
       " u'alphabet',\n",
       " u'civil',\n",
       " u'italian',\n",
       " u'peninsula',\n",
       " u'left',\n",
       " u'letter',\n",
       " u'unchang',\n",
       " u'roman',\n",
       " u'later',\n",
       " u'adopt',\n",
       " u'etruscan',\n",
       " u'alphabet',\n",
       " u'write',\n",
       " u'latin',\n",
       " u'languag',\n",
       " u'result',\n",
       " u'letter',\n",
       " u'preserv',\n",
       " u'latin',\n",
       " u'alphabet',\n",
       " u'use',\n",
       " u'write',\n",
       " u'mani',\n",
       " u'languag',\n",
       " u'includ',\n",
       " u'english',\n",
       " u'roman',\n",
       " u'time',\n",
       " u'mani',\n",
       " u'variat',\n",
       " u'letter',\n",
       " u'first',\n",
       " u'monument',\n",
       " u'lapidari',\n",
       " u'style',\n",
       " u'use',\n",
       " u'inscrib',\n",
       " u'stone',\n",
       " u'perman',\n",
       " u'medium',\n",
       " u'perish',\n",
       " u'surfac',\n",
       " u'use',\n",
       " u'everyday',\n",
       " u'utilitarian',\n",
       " u'purpos']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['290']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build an inverted index for the documents, so that we can quickly access documents for the terms we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building an inverted index for the documents\n",
    "\n",
    "from collections import defaultdict\n",
    "    \n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "for docid, terms in documents.items():\n",
    "    for term in terms:\n",
    "        inverted_index[term].add(docid)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test it out, the list of documents containing the word 'ship':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100011',\n",
       " '10046',\n",
       " '10049',\n",
       " '100782',\n",
       " '101347',\n",
       " '101906',\n",
       " '102231',\n",
       " '10482',\n",
       " '106130',\n",
       " '106574',\n",
       " '106675',\n",
       " '106806',\n",
       " '107334',\n",
       " '107511',\n",
       " '10887',\n",
       " '108989',\n",
       " '109267',\n",
       " '109347',\n",
       " '10937',\n",
       " '10969',\n",
       " '110062',\n",
       " '110499',\n",
       " '110956',\n",
       " '112631',\n",
       " '113565',\n",
       " '11464',\n",
       " '115072',\n",
       " '115504',\n",
       " '116754',\n",
       " '11719',\n",
       " '118501',\n",
       " '120142',\n",
       " '12233',\n",
       " '122926',\n",
       " '123038',\n",
       " '12514',\n",
       " '127579',\n",
       " '127698',\n",
       " '128254',\n",
       " '129232',\n",
       " '129509',\n",
       " '131855',\n",
       " '132410',\n",
       " '133932',\n",
       " '13395',\n",
       " '13475',\n",
       " '1358',\n",
       " '136933',\n",
       " '137573',\n",
       " '138169',\n",
       " '13830',\n",
       " '140725',\n",
       " '140794',\n",
       " '14090',\n",
       " '14092',\n",
       " '140976',\n",
       " '141008',\n",
       " '14105',\n",
       " '142261',\n",
       " '143062',\n",
       " '14314',\n",
       " '143294',\n",
       " '143817',\n",
       " '144598',\n",
       " '1446',\n",
       " '14532',\n",
       " '145352',\n",
       " '145415',\n",
       " '145567',\n",
       " '145672',\n",
       " '145773',\n",
       " '146635',\n",
       " '146640',\n",
       " '147581',\n",
       " '147610',\n",
       " '147972',\n",
       " '147997',\n",
       " '148126',\n",
       " '14849',\n",
       " '149342',\n",
       " '149404',\n",
       " '149708',\n",
       " '14986',\n",
       " '150473',\n",
       " '151160',\n",
       " '151211',\n",
       " '151453',\n",
       " '151497',\n",
       " '151630',\n",
       " '151663',\n",
       " '152061',\n",
       " '152792',\n",
       " '152847',\n",
       " '152952',\n",
       " '153318',\n",
       " '15357',\n",
       " '153850',\n",
       " '153942',\n",
       " '154310',\n",
       " '154456',\n",
       " '154552',\n",
       " '154815',\n",
       " '154954',\n",
       " '155747',\n",
       " '156018',\n",
       " '156275',\n",
       " '156767',\n",
       " '156787',\n",
       " '158128',\n",
       " '158364',\n",
       " '158641',\n",
       " '159811',\n",
       " '159961',\n",
       " '160420',\n",
       " '160519',\n",
       " '160914',\n",
       " '16142',\n",
       " '16167',\n",
       " '161768',\n",
       " '162241',\n",
       " '162745',\n",
       " '16341',\n",
       " '164340',\n",
       " '16440',\n",
       " '164474',\n",
       " '164508',\n",
       " '165228',\n",
       " '16558',\n",
       " '165723',\n",
       " '166115',\n",
       " '166357',\n",
       " '166570',\n",
       " '16674',\n",
       " '166842',\n",
       " '167446',\n",
       " '16774',\n",
       " '167741',\n",
       " '16782',\n",
       " '169903',\n",
       " '170023',\n",
       " '170270',\n",
       " '170569',\n",
       " '17125',\n",
       " '171507',\n",
       " '172121',\n",
       " '172237',\n",
       " '173082',\n",
       " '173394',\n",
       " '173857',\n",
       " '17386',\n",
       " '174818',\n",
       " '175571',\n",
       " '176324',\n",
       " '176423',\n",
       " '176544',\n",
       " '176781',\n",
       " '177221',\n",
       " '178069',\n",
       " '178101',\n",
       " '178269',\n",
       " '178300',\n",
       " '178410',\n",
       " '178635',\n",
       " '178876',\n",
       " '1790',\n",
       " '180028',\n",
       " '180486',\n",
       " '180697',\n",
       " '180997',\n",
       " '181160',\n",
       " '18138',\n",
       " '181418',\n",
       " '181567',\n",
       " '182236',\n",
       " '182544',\n",
       " '182664',\n",
       " '183082',\n",
       " '183609',\n",
       " '183913',\n",
       " '18402',\n",
       " '184046',\n",
       " '184170',\n",
       " '18432',\n",
       " '184399',\n",
       " '1844',\n",
       " '185235',\n",
       " '186252',\n",
       " '188086',\n",
       " '188545',\n",
       " '188554',\n",
       " '188590',\n",
       " '189252',\n",
       " '189509',\n",
       " '190389',\n",
       " '190847',\n",
       " '191156',\n",
       " '19147',\n",
       " '191513',\n",
       " '192108',\n",
       " '192227',\n",
       " '192755',\n",
       " '193169',\n",
       " '193416',\n",
       " '194856',\n",
       " '195137',\n",
       " '195259',\n",
       " '19545',\n",
       " '19579',\n",
       " '195848',\n",
       " '195883',\n",
       " '19594',\n",
       " '195952',\n",
       " '196193',\n",
       " '196637',\n",
       " '19708',\n",
       " '197127',\n",
       " '197232',\n",
       " '19745',\n",
       " '197973',\n",
       " '198006',\n",
       " '198016',\n",
       " '19812',\n",
       " '198146',\n",
       " '198199',\n",
       " '198201',\n",
       " '19859',\n",
       " '198616',\n",
       " '198856',\n",
       " '19898',\n",
       " '199109',\n",
       " '199506',\n",
       " '199520',\n",
       " '199742',\n",
       " '200333',\n",
       " '200386',\n",
       " '201487',\n",
       " '201877',\n",
       " '20206',\n",
       " '202696',\n",
       " '203256',\n",
       " '203459',\n",
       " '203872',\n",
       " '20388',\n",
       " '205006',\n",
       " '205162',\n",
       " '205392',\n",
       " '205394',\n",
       " '205550',\n",
       " '205842',\n",
       " '206552',\n",
       " '206772',\n",
       " '206932',\n",
       " '20736',\n",
       " '208264',\n",
       " '208885',\n",
       " '208897',\n",
       " '209561',\n",
       " '210074',\n",
       " '210155',\n",
       " '211233',\n",
       " '211540',\n",
       " '212073',\n",
       " '212076',\n",
       " '213144',\n",
       " '213739',\n",
       " '215108',\n",
       " '215139',\n",
       " '21533',\n",
       " '215377',\n",
       " '216017',\n",
       " '216361',\n",
       " '216434',\n",
       " '21650',\n",
       " '21685',\n",
       " '21694',\n",
       " '217553',\n",
       " '21790',\n",
       " '21851',\n",
       " '21854',\n",
       " '21871',\n",
       " '21891',\n",
       " '219797',\n",
       " '22018',\n",
       " '22039',\n",
       " '22102',\n",
       " '221315',\n",
       " '221376',\n",
       " '221430',\n",
       " '22151',\n",
       " '221725',\n",
       " '2219',\n",
       " '222959',\n",
       " '226043',\n",
       " '22703',\n",
       " '227232',\n",
       " '229073',\n",
       " '231223',\n",
       " '232459',\n",
       " '232522',\n",
       " '233146',\n",
       " '233403',\n",
       " '234687',\n",
       " '235279',\n",
       " '236466',\n",
       " '236482',\n",
       " '236487',\n",
       " '237752',\n",
       " '238244',\n",
       " '238512',\n",
       " '239677',\n",
       " '239916',\n",
       " '24080',\n",
       " '24261',\n",
       " '243373',\n",
       " '243674',\n",
       " '24390',\n",
       " '246155',\n",
       " '246257',\n",
       " '247400',\n",
       " '247409',\n",
       " '24742',\n",
       " '247647',\n",
       " '248051',\n",
       " '248946',\n",
       " '248962',\n",
       " '250499',\n",
       " '250637',\n",
       " '251306',\n",
       " '251489',\n",
       " '25237',\n",
       " '252487',\n",
       " '253375',\n",
       " '253767',\n",
       " '254083',\n",
       " '2547',\n",
       " '254814',\n",
       " '255109',\n",
       " '255286',\n",
       " '2563',\n",
       " '256708',\n",
       " '25676',\n",
       " '256928',\n",
       " '25715',\n",
       " '257179',\n",
       " '257290',\n",
       " '257337',\n",
       " '25866',\n",
       " '259417',\n",
       " '26061',\n",
       " '26072',\n",
       " '260869',\n",
       " '261091',\n",
       " '261096',\n",
       " '26122',\n",
       " '261293',\n",
       " '261703',\n",
       " '262293',\n",
       " '262577',\n",
       " '2628',\n",
       " '263276',\n",
       " '263960',\n",
       " '26417',\n",
       " '264285',\n",
       " '264853',\n",
       " '265248',\n",
       " '266311',\n",
       " '266443',\n",
       " '267918',\n",
       " '269265',\n",
       " '269326',\n",
       " '26970',\n",
       " '27008',\n",
       " '27058',\n",
       " '270849',\n",
       " '271016',\n",
       " '27218',\n",
       " '272729',\n",
       " '273234',\n",
       " '27380',\n",
       " '275540',\n",
       " '276170',\n",
       " '27643',\n",
       " '276608',\n",
       " '27672',\n",
       " '276844',\n",
       " '276852',\n",
       " '277285',\n",
       " '277289',\n",
       " '277565',\n",
       " '27881',\n",
       " '27888',\n",
       " '282291',\n",
       " '28237',\n",
       " '28238',\n",
       " '28239',\n",
       " '28266',\n",
       " '28377',\n",
       " '284629',\n",
       " '285391',\n",
       " '285586',\n",
       " '285868',\n",
       " '285891',\n",
       " '28617',\n",
       " '286400',\n",
       " '287759',\n",
       " '28866',\n",
       " '28953',\n",
       " '289813',\n",
       " '290279',\n",
       " '29091',\n",
       " '291973',\n",
       " '292402',\n",
       " '29307',\n",
       " '29323',\n",
       " '293264',\n",
       " '293463',\n",
       " '295078',\n",
       " '295712',\n",
       " '297673',\n",
       " '299497',\n",
       " '300063',\n",
       " '300489',\n",
       " '30072',\n",
       " '30178',\n",
       " '302622',\n",
       " '305224',\n",
       " '305370',\n",
       " '306085',\n",
       " '306908',\n",
       " '30698',\n",
       " '30775',\n",
       " '3080',\n",
       " '308017',\n",
       " '308069',\n",
       " '308211',\n",
       " '308643',\n",
       " '308864',\n",
       " '309191',\n",
       " '30936',\n",
       " '309759',\n",
       " '310547',\n",
       " '31064',\n",
       " '31153',\n",
       " '311643',\n",
       " '311663',\n",
       " '31199',\n",
       " '312222',\n",
       " '31238',\n",
       " '31294',\n",
       " '313147',\n",
       " '314046',\n",
       " '31424',\n",
       " '314269',\n",
       " '31456',\n",
       " '3146',\n",
       " '3152',\n",
       " '316134',\n",
       " '316353',\n",
       " '317599',\n",
       " '318138',\n",
       " '318147',\n",
       " '318209',\n",
       " '31888',\n",
       " '319558',\n",
       " '319568',\n",
       " '3218',\n",
       " '32193',\n",
       " '322110',\n",
       " '323691',\n",
       " '324741',\n",
       " '32538',\n",
       " '326003',\n",
       " '32610',\n",
       " '326524',\n",
       " '326837',\n",
       " '32699',\n",
       " '327487',\n",
       " '328029',\n",
       " '328107',\n",
       " '328288',\n",
       " '328399',\n",
       " '328576',\n",
       " '328592',\n",
       " '33174',\n",
       " '33787',\n",
       " '34059',\n",
       " '34064',\n",
       " '34159',\n",
       " '34190',\n",
       " '3451',\n",
       " '3455',\n",
       " '3456',\n",
       " '36681',\n",
       " '36875',\n",
       " '3705',\n",
       " '37146',\n",
       " '3742',\n",
       " '38025',\n",
       " '38031',\n",
       " '38149',\n",
       " '38441',\n",
       " '38855',\n",
       " '39766',\n",
       " '4005',\n",
       " '4012',\n",
       " '40149',\n",
       " '4015',\n",
       " '40410',\n",
       " '4054',\n",
       " '40909',\n",
       " '4177',\n",
       " '41885',\n",
       " '41962',\n",
       " '42387',\n",
       " '42418',\n",
       " '42478',\n",
       " '42737',\n",
       " '43281',\n",
       " '43415',\n",
       " '43416',\n",
       " '43596',\n",
       " '43927',\n",
       " '44014',\n",
       " '44252',\n",
       " '44920',\n",
       " '4501',\n",
       " '45207',\n",
       " '45835',\n",
       " '45908',\n",
       " '46149',\n",
       " '46417',\n",
       " '47188',\n",
       " '47898',\n",
       " '47968',\n",
       " '4806',\n",
       " '48442',\n",
       " '48607',\n",
       " '4876',\n",
       " '49076',\n",
       " '49200',\n",
       " '49609',\n",
       " '49702',\n",
       " '49728',\n",
       " '49824',\n",
       " '49827',\n",
       " '50016',\n",
       " '5008',\n",
       " '5034',\n",
       " '50548',\n",
       " '50784',\n",
       " '51011',\n",
       " '51049',\n",
       " '52027',\n",
       " '52371',\n",
       " '52616',\n",
       " '53117',\n",
       " '53289',\n",
       " '53839',\n",
       " '5408',\n",
       " '54253',\n",
       " '5500',\n",
       " '55004',\n",
       " '55106',\n",
       " '55664',\n",
       " '55759',\n",
       " '56092',\n",
       " '56196',\n",
       " '5623',\n",
       " '56494',\n",
       " '56565',\n",
       " '56630',\n",
       " '56727',\n",
       " '56836',\n",
       " '5685',\n",
       " '57021',\n",
       " '57157',\n",
       " '57646',\n",
       " '57690',\n",
       " '57708',\n",
       " '57765',\n",
       " '57835',\n",
       " '57849',\n",
       " '57930',\n",
       " '57969',\n",
       " '58268',\n",
       " '58664',\n",
       " '58716',\n",
       " '58909',\n",
       " '58916',\n",
       " '59104',\n",
       " '59198',\n",
       " '59373',\n",
       " '5956',\n",
       " '59592',\n",
       " '59599',\n",
       " '59827',\n",
       " '60004',\n",
       " '60018',\n",
       " '60098',\n",
       " '60145',\n",
       " '60646',\n",
       " '60912',\n",
       " '60982',\n",
       " '61164',\n",
       " '61732',\n",
       " '61976',\n",
       " '61992',\n",
       " '62277',\n",
       " '62390',\n",
       " '62437',\n",
       " '62713',\n",
       " '62717',\n",
       " '62952',\n",
       " '63065',\n",
       " '63278',\n",
       " '63375',\n",
       " '63972',\n",
       " '65064',\n",
       " '65192',\n",
       " '6555',\n",
       " '66054',\n",
       " '66068',\n",
       " '66086',\n",
       " '6653',\n",
       " '66662',\n",
       " '66982',\n",
       " '67019',\n",
       " '67030',\n",
       " '67032',\n",
       " '67234',\n",
       " '67417',\n",
       " '67675',\n",
       " '67764',\n",
       " '67806',\n",
       " '67896',\n",
       " '67990',\n",
       " '68418',\n",
       " '69162',\n",
       " '69301',\n",
       " '69398',\n",
       " '69406',\n",
       " '69443',\n",
       " '69733',\n",
       " '69798',\n",
       " '69905',\n",
       " '69916',\n",
       " '69972',\n",
       " '70024',\n",
       " '70061',\n",
       " '7034',\n",
       " '70566',\n",
       " '70667',\n",
       " '71153',\n",
       " '71182',\n",
       " '72331',\n",
       " '72336',\n",
       " '72850',\n",
       " '73212',\n",
       " '73330',\n",
       " '74209',\n",
       " '74337',\n",
       " '74374',\n",
       " '76086',\n",
       " '7701',\n",
       " '77118',\n",
       " '77396',\n",
       " '77444',\n",
       " '77658',\n",
       " '77733',\n",
       " '77735',\n",
       " '77856',\n",
       " '77965',\n",
       " '78189',\n",
       " '78579',\n",
       " '7923',\n",
       " '79258',\n",
       " '79682',\n",
       " '80187',\n",
       " '80313',\n",
       " '80522',\n",
       " '8073',\n",
       " '81915',\n",
       " '81943',\n",
       " '81949',\n",
       " '82001',\n",
       " '82030',\n",
       " '82068',\n",
       " '8209',\n",
       " '82156',\n",
       " '82176',\n",
       " '82481',\n",
       " '83582',\n",
       " '83583',\n",
       " '8408',\n",
       " '84205',\n",
       " '84524',\n",
       " '84674',\n",
       " '849',\n",
       " '85378',\n",
       " '87610',\n",
       " '8779',\n",
       " '88259',\n",
       " '8839',\n",
       " '88718',\n",
       " '89514',\n",
       " '89821',\n",
       " '89866',\n",
       " '900',\n",
       " '90451',\n",
       " '93060',\n",
       " '93099',\n",
       " '93454',\n",
       " '94240',\n",
       " '94609',\n",
       " '9541',\n",
       " '95421',\n",
       " '95868',\n",
       " '96875',\n",
       " '97169',\n",
       " '9845',\n",
       " '98508',\n",
       " '9932',\n",
       " '9961',\n",
       " '99633'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index['ship']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to the BM25 TF-IDF representation, we'll create the td-idf matrix for terms-documents, first without the query component. \n",
    "\n",
    "The query component is dependent on the terms in our query. So we'll just calculate that, and multiply it with the overall score when we want to retreive documents for a particular query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building a TF-IDF representation using BM25 \n",
    "\n",
    "NO_DOCS = len(documents) #Number of documents\n",
    "\n",
    "AVG_LEN_DOC = sum([len(doc) for doc in documents.values()])/len(documents) #Average length of documents\n",
    "\n",
    "#The function below takes the documentid, and the term, to calculate scores for the tf and idf\n",
    "#components, and multiplies them together.\n",
    "def tf_idf_score(k1,b,term,docid):  \n",
    "    \n",
    "    ft = len(inverted_index[term]) \n",
    "    term = stemmer.stem(term.lower())\n",
    "    fdt =  documents[docid].count(term)\n",
    "    \n",
    "    idf_comp = math.log((NO_DOCS - ft + 0.5)/(ft+0.5))\n",
    "    \n",
    "    tf_comp = ((k1 + 1)*fdt)/(k1*((1-b) + b*(len(documents[docid])/AVG_LEN_DOC))+fdt)\n",
    "    \n",
    "    return idf_comp * tf_comp\n",
    "\n",
    "#Method to create tf_idf matrix without the query component\n",
    "def create_tf_idf(k1,b):\n",
    "    tf_idf = defaultdict(dict)\n",
    "    for term in set(inverted_index.keys()):\n",
    "        for docid in inverted_index[term]:\n",
    "            tf_idf[term][docid] = tf_idf_score(k1,b,term,docid)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating tf_idf matrix with said parameter values: k1 and b for all documents.\n",
    "tf_idf = create_tf_idf(1.5,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the default values for k1 and b (1.5 and 0.5), which seemed to give good results. Although these parameters may be altered depending on the type of data being dealth with. \n",
    "\n",
    "Now we create a method to retrieve the query component, and another method that will use the previous ones and retrieve the relevant documents for a query, sorted on the basis of their ranks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method to retrieve query component\n",
    "def get_qtf_comp(k3,term,fqt):\n",
    "    return ((k3+1)*fqt[term])/(k3 + fqt[term])\n",
    "\n",
    "\n",
    "#Method to retrieve documents || Returns a set of documents and their relevance scores. \n",
    "def retr_docs(query,result_count):\n",
    "    q_terms = [stemmer.stem(term.lower()) for term in query.split() if term not in stopwords] #Removing stopwords from queries\n",
    "    fqt = {}\n",
    "    for term in q_terms:\n",
    "        fqt[term] = fqt.get(term,0) + 1\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for word in fqt.keys():\n",
    "        #print word + ': '+ str(inverted_index[word])\n",
    "        for document in inverted_index[word]:\n",
    "            scores[document] = scores.get(document,0) + (tf_idf[word][document]*get_qtf_comp(0,word,fqt)) #k3 chosen as 0 (default)\n",
    "    \n",
    "    return sorted(scores.items(),key = lambda x : x[1] , reverse=True)[:result_count]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and retrieve a document for a query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('19961', 12.570721363284687),\n",
       " ('83266', 12.500367334396838),\n",
       " ('266959', 12.46418348068098),\n",
       " ('20206', 12.324327863972716),\n",
       " ('253314', 12.008548114449386)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retr_docs(\"Manchester United\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the terms in the top ranked document.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'manchest',\n",
       " u'unit',\n",
       " u'manchest',\n",
       " u'unit',\n",
       " u'footbal',\n",
       " u'club',\n",
       " u'english',\n",
       " u'profession',\n",
       " u'footbal',\n",
       " u'club',\n",
       " u'base',\n",
       " u'old',\n",
       " u'trafford',\n",
       " u'greater',\n",
       " u'manchest',\n",
       " u'play',\n",
       " u'premier',\n",
       " u'leagu',\n",
       " u'found',\n",
       " u'newton',\n",
       " u'heath',\n",
       " u'lyr',\n",
       " u'footbal',\n",
       " u'club',\n",
       " u'club',\n",
       " u'chang',\n",
       " u'name',\n",
       " u'manchest',\n",
       " u'unit',\n",
       " u'move',\n",
       " u'old',\n",
       " u'trafford',\n",
       " u'manchest',\n",
       " u'unit',\n",
       " u'mani',\n",
       " u'trophi',\n",
       " u'english',\n",
       " u'footbal',\n",
       " u'includ',\n",
       " u'record',\n",
       " u'leagu',\n",
       " u'titl',\n",
       " u'record',\n",
       " u'fa',\n",
       " u'cup',\n",
       " u'four',\n",
       " u'leagu',\n",
       " u'cup',\n",
       " u'record',\n",
       " u'fa',\n",
       " u'commun',\n",
       " u'shield',\n",
       " u'club',\n",
       " u'also',\n",
       " u'three',\n",
       " u'european',\n",
       " u'cup',\n",
       " u'one',\n",
       " u'uefa',\n",
       " u'cup',\n",
       " u'winner',\n",
       " u'cup',\n",
       " u'one',\n",
       " u'uefa',\n",
       " u'super',\n",
       " u'cup',\n",
       " u'one',\n",
       " u'intercontinent',\n",
       " u'cup',\n",
       " u'one',\n",
       " u'fifa',\n",
       " u'club',\n",
       " u'world',\n",
       " u'cup',\n",
       " u'--',\n",
       " u'club',\n",
       " u'continent',\n",
       " u'trebl',\n",
       " u'premier',\n",
       " u'leagu',\n",
       " u'fa',\n",
       " u'cup',\n",
       " u'uefa',\n",
       " u'champion',\n",
       " u'leagu',\n",
       " u'unpreced',\n",
       " u'feat',\n",
       " u'english',\n",
       " u'club',\n",
       " u'munich',\n",
       " u'air',\n",
       " u'disast',\n",
       " u'claim',\n",
       " u'live',\n",
       " u'eight',\n",
       " u'player',\n",
       " u'manag',\n",
       " u'matt',\n",
       " u'busbi',\n",
       " u'manchest',\n",
       " u'unit',\n",
       " u'first',\n",
       " u'english',\n",
       " u'footbal',\n",
       " u'club',\n",
       " u'win',\n",
       " u'european',\n",
       " u'cup',\n",
       " u'alex',\n",
       " u'ferguson',\n",
       " u'major',\n",
       " u'honour',\n",
       " u'total',\n",
       " u'novemb',\n",
       " u'may',\n",
       " u'announc',\n",
       " u'retir',\n",
       " u'year',\n",
       " u'club',\n",
       " u'fellow',\n",
       " u'scot',\n",
       " u'david',\n",
       " u'moy',\n",
       " u'appoint',\n",
       " u'replac',\n",
       " u'may',\n",
       " u'manchest',\n",
       " u'unit',\n",
       " u'third',\n",
       " u'richest',\n",
       " u'footbal',\n",
       " u'club',\n",
       " u'world',\n",
       " u'--',\n",
       " u'term',\n",
       " u'revenu',\n",
       " u'annual',\n",
       " u'revenu',\n",
       " u'million',\n",
       " u'second',\n",
       " u'valuabl',\n",
       " u'club',\n",
       " u'valu',\n",
       " u'billion',\n",
       " u'one',\n",
       " u'wide',\n",
       " u'support',\n",
       " u'footbal',\n",
       " u'team',\n",
       " u'world',\n",
       " u'float',\n",
       " u'london',\n",
       " u'stock',\n",
       " u'exchang',\n",
       " u'club',\n",
       " u'purchas',\n",
       " u'malcolm',\n",
       " u'glazer',\n",
       " u'may',\n",
       " u'deal',\n",
       " u'valu',\n",
       " u'club',\n",
       " u'almost',\n",
       " u'gbp',\n",
       " u'million',\n",
       " u'august',\n",
       " u'manchest',\n",
       " u'unit',\n",
       " u'made',\n",
       " u'initi',\n",
       " u'public',\n",
       " u'offer',\n",
       " u'new',\n",
       " u'york',\n",
       " u'stock',\n",
       " u'exchang',\n",
       " u'histori',\n",
       " u'earli',\n",
       " u'year',\n",
       " u'--',\n",
       " u'manchest',\n",
       " u'unit',\n",
       " u'form',\n",
       " u'newton',\n",
       " u'heath',\n",
       " u'lyr',\n",
       " u'footbal',\n",
       " u'club',\n",
       " u'carriag']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['19961']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find out the the retrieval engine has worked quite well in this case. The top ranked document for the query is a snippet of the wikipedia article for Manchester United Football Club. \n",
    "\n",
    "On further inspection, we can see that the documents ranked lower are, for example, for The University of Manchester, or even just articles with the words 'Manchester' or 'United' in them.\n",
    "\n",
    "Now we can begin translating the German queries to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
