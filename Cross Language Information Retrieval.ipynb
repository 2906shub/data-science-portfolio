{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross Language Information Retrieval\n",
    "\n",
    "#### Overview\n",
    "\n",
    "The aim of this project is to build a cross language information retrieval system (CLIR) which, given a query in German, will be capable of searching text documents written in English and displaying the results in German.\n",
    "\n",
    "We're going to use machine translation, information retrieval using a vector space model, and then assess the performance of the system using IR evaluation techniques.\n",
    "\n",
    "Parts of the project are explained as we progress.\n",
    "\n",
    "#### Data Used\n",
    "\n",
    "- bitext.(en,de): A sentence aligned, parallel German-English corpus, sourced from the Europarl corpus (which is a collection of debates held in the EU parliament over a number of years). We'll use this to develop word-alignment tools, and build a translation probability table. \n",
    "\n",
    "- newstest.(en,de): A separate, smaller parallel corpus for evaulation of the translation system.\n",
    "\n",
    "- devel.(docs,queries,qrel): A set of documents in English (sourced from Wikipedia), queries in German, and relevance judgement scores for each query-document pair. \n",
    "\n",
    "The files are available to check out in the data/clir directory of the repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping: File encodings and tokenisation\n",
    "\n",
    "Since the data files we use is utf-8 encoded text, we need to convert the strings into ASCII by escaping the special symbols. We also import some libraries in this step as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from __future__ import division #To properly handle floating point divisions.\n",
    "import math\n",
    "\n",
    "def tokenize(line, tokenizer=word_tokenize):\n",
    "    utf_line = line.decode('utf-8').lower()\n",
    "    return [token.encode('ascii', 'backslashreplace') for token in tokenizer(utf_line)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test out our tokenize function. Notice how it converts the word Über."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seit',\n",
       " 'damals',\n",
       " 'ist',\n",
       " 'er',\n",
       " 'auf',\n",
       " '\\\\xfcber',\n",
       " '10.000',\n",
       " 'punkte',\n",
       " 'gestiegen',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Seit damals ist er auf über 10.000 Punkte gestiegen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, lets get to the meat of the project. \n",
    "\n",
    "As mentioned earlier, we're going to build a CLIR engine consisting of information retrieval and translation components, and then evaluate its accuracy.\n",
    "\n",
    "The CLIR system will:\n",
    "- **translate queries** from German into English (because our searcheable corpus is in English), using word-based translation, a rather simplistic approach as opposed to the sophistication you might see in, say, *Google Translate*.\n",
    "- **search over the document corpus** using the Okapi BM25 IR ranking model, a variation of the traditional TF-IDF model.\n",
    "- **evaluate the quality** of ranked retrieval results using the query relevance judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval using [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25)\n",
    "\n",
    "We'll start by building an IR system, and give it a test run with some English queries. \n",
    "\n",
    "Here's an overview of the tasks involved:\n",
    "- Loading the data files, and tokenizing the input.\n",
    "- Preprocessing the lexicon by stemming, removing stopwords.\n",
    "- Calculating the TF/IDF representation for all documents in our wikipedia corpus.\n",
    "- Storing an inverted index to efficiently documents, given a query term.\n",
    "- Implementing querying with BM25.\n",
    "- Test runs.\n",
    "\n",
    "So for our first task, we'll load the devel.docs file, extract and tokenize the terms, and store them in a python dictionary with the document ids as keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
